{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Netwok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvNet\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision \n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.transforms as transforms \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "\n",
    "# device config \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# hyper parameters \n",
    "num_epochs = 4 \n",
    "batch_size = 4 \n",
    "learning_rate = 0.001 \n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                               transforms.Normalize((0.5, 0.5,0.5),(0.5, 0.5 ,0.5))])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root = './data', train= True, transform= transforms.ToTensor(),download=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root = './data', train= False, transform= transforms.ToTensor(),download=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Step [100/12500], Loss: 2.2990\n",
      "Epoch [1/4], Step [200/12500], Loss: 2.2807\n",
      "Epoch [1/4], Step [300/12500], Loss: 2.2752\n",
      "Epoch [1/4], Step [400/12500], Loss: 2.3530\n",
      "Epoch [1/4], Step [500/12500], Loss: 2.3087\n",
      "Epoch [1/4], Step [600/12500], Loss: 2.3620\n",
      "Epoch [1/4], Step [700/12500], Loss: 2.2716\n",
      "Epoch [1/4], Step [800/12500], Loss: 2.3867\n",
      "Epoch [1/4], Step [900/12500], Loss: 2.3355\n",
      "Epoch [1/4], Step [1000/12500], Loss: 2.3060\n",
      "Epoch [1/4], Step [1100/12500], Loss: 2.2789\n",
      "Epoch [1/4], Step [1200/12500], Loss: 2.2880\n",
      "Epoch [1/4], Step [1300/12500], Loss: 2.3141\n",
      "Epoch [1/4], Step [1400/12500], Loss: 2.2923\n",
      "Epoch [1/4], Step [1500/12500], Loss: 2.2807\n",
      "Epoch [1/4], Step [1600/12500], Loss: 2.2729\n",
      "Epoch [1/4], Step [1700/12500], Loss: 2.2879\n",
      "Epoch [1/4], Step [1800/12500], Loss: 2.2762\n",
      "Epoch [1/4], Step [1900/12500], Loss: 2.2958\n",
      "Epoch [1/4], Step [2000/12500], Loss: 2.2889\n",
      "Epoch [1/4], Step [2100/12500], Loss: 2.2699\n",
      "Epoch [1/4], Step [2200/12500], Loss: 2.2973\n",
      "Epoch [1/4], Step [2300/12500], Loss: 2.3492\n",
      "Epoch [1/4], Step [2400/12500], Loss: 2.2733\n",
      "Epoch [1/4], Step [2500/12500], Loss: 2.2883\n",
      "Epoch [1/4], Step [2600/12500], Loss: 2.3290\n",
      "Epoch [1/4], Step [2700/12500], Loss: 2.3674\n",
      "Epoch [1/4], Step [2800/12500], Loss: 2.3139\n",
      "Epoch [1/4], Step [2900/12500], Loss: 2.2598\n",
      "Epoch [1/4], Step [3000/12500], Loss: 2.2927\n",
      "Epoch [1/4], Step [3100/12500], Loss: 2.2838\n",
      "Epoch [1/4], Step [3200/12500], Loss: 2.3036\n",
      "Epoch [1/4], Step [3300/12500], Loss: 2.2925\n",
      "Epoch [1/4], Step [3400/12500], Loss: 2.3083\n",
      "Epoch [1/4], Step [3500/12500], Loss: 2.2766\n",
      "Epoch [1/4], Step [3600/12500], Loss: 2.2927\n",
      "Epoch [1/4], Step [3700/12500], Loss: 2.3133\n",
      "Epoch [1/4], Step [3800/12500], Loss: 2.2644\n",
      "Epoch [1/4], Step [3900/12500], Loss: 2.3193\n",
      "Epoch [1/4], Step [4000/12500], Loss: 2.3198\n",
      "Epoch [1/4], Step [4100/12500], Loss: 2.3277\n",
      "Epoch [1/4], Step [4200/12500], Loss: 2.3281\n",
      "Epoch [1/4], Step [4300/12500], Loss: 2.3165\n",
      "Epoch [1/4], Step [4400/12500], Loss: 2.3050\n",
      "Epoch [1/4], Step [4500/12500], Loss: 2.3399\n",
      "Epoch [1/4], Step [4600/12500], Loss: 2.3084\n",
      "Epoch [1/4], Step [4700/12500], Loss: 2.2761\n",
      "Epoch [1/4], Step [4800/12500], Loss: 2.3137\n",
      "Epoch [1/4], Step [4900/12500], Loss: 2.3118\n",
      "Epoch [1/4], Step [5000/12500], Loss: 2.3278\n",
      "Epoch [1/4], Step [5100/12500], Loss: 2.3004\n",
      "Epoch [1/4], Step [5200/12500], Loss: 2.2856\n",
      "Epoch [1/4], Step [5300/12500], Loss: 2.3129\n",
      "Epoch [1/4], Step [5400/12500], Loss: 2.2830\n",
      "Epoch [1/4], Step [5500/12500], Loss: 2.2979\n",
      "Epoch [1/4], Step [5600/12500], Loss: 2.3061\n",
      "Epoch [1/4], Step [5700/12500], Loss: 2.3354\n",
      "Epoch [1/4], Step [5800/12500], Loss: 2.2901\n",
      "Epoch [1/4], Step [5900/12500], Loss: 2.3158\n",
      "Epoch [1/4], Step [6000/12500], Loss: 2.2901\n",
      "Epoch [1/4], Step [6100/12500], Loss: 2.2812\n",
      "Epoch [1/4], Step [6200/12500], Loss: 2.2760\n",
      "Epoch [1/4], Step [6300/12500], Loss: 2.2660\n",
      "Epoch [1/4], Step [6400/12500], Loss: 2.2778\n",
      "Epoch [1/4], Step [6500/12500], Loss: 2.2891\n",
      "Epoch [1/4], Step [6600/12500], Loss: 2.2980\n",
      "Epoch [1/4], Step [6700/12500], Loss: 2.3384\n",
      "Epoch [1/4], Step [6800/12500], Loss: 2.2728\n",
      "Epoch [1/4], Step [6900/12500], Loss: 2.3238\n",
      "Epoch [1/4], Step [7000/12500], Loss: 2.3135\n",
      "Epoch [1/4], Step [7100/12500], Loss: 2.3048\n",
      "Epoch [1/4], Step [7200/12500], Loss: 2.3254\n",
      "Epoch [1/4], Step [7300/12500], Loss: 2.3187\n",
      "Epoch [1/4], Step [7400/12500], Loss: 2.3013\n",
      "Epoch [1/4], Step [7500/12500], Loss: 2.2700\n",
      "Epoch [1/4], Step [7600/12500], Loss: 2.3104\n",
      "Epoch [1/4], Step [7700/12500], Loss: 2.2988\n",
      "Epoch [1/4], Step [7800/12500], Loss: 2.3230\n",
      "Epoch [1/4], Step [7900/12500], Loss: 2.2965\n",
      "Epoch [1/4], Step [8000/12500], Loss: 2.2989\n",
      "Epoch [1/4], Step [8100/12500], Loss: 2.3084\n",
      "Epoch [1/4], Step [8200/12500], Loss: 2.3174\n",
      "Epoch [1/4], Step [8300/12500], Loss: 2.3014\n",
      "Epoch [1/4], Step [8400/12500], Loss: 2.2908\n",
      "Epoch [1/4], Step [8500/12500], Loss: 2.2915\n",
      "Epoch [1/4], Step [8600/12500], Loss: 2.3088\n",
      "Epoch [1/4], Step [8700/12500], Loss: 2.2929\n",
      "Epoch [1/4], Step [8800/12500], Loss: 2.3259\n",
      "Epoch [1/4], Step [8900/12500], Loss: 2.2884\n",
      "Epoch [1/4], Step [9000/12500], Loss: 2.3052\n",
      "Epoch [1/4], Step [9100/12500], Loss: 2.3059\n",
      "Epoch [1/4], Step [9200/12500], Loss: 2.2984\n",
      "Epoch [1/4], Step [9300/12500], Loss: 2.2930\n",
      "Epoch [1/4], Step [9400/12500], Loss: 2.3290\n",
      "Epoch [1/4], Step [9500/12500], Loss: 2.3107\n",
      "Epoch [1/4], Step [9600/12500], Loss: 2.3065\n",
      "Epoch [1/4], Step [9700/12500], Loss: 2.2960\n",
      "Epoch [1/4], Step [9800/12500], Loss: 2.2873\n",
      "Epoch [1/4], Step [9900/12500], Loss: 2.2870\n",
      "Epoch [1/4], Step [10000/12500], Loss: 2.2948\n",
      "Epoch [1/4], Step [10100/12500], Loss: 2.2956\n",
      "Epoch [1/4], Step [10200/12500], Loss: 2.3094\n",
      "Epoch [1/4], Step [10300/12500], Loss: 2.3085\n",
      "Epoch [1/4], Step [10400/12500], Loss: 2.3086\n",
      "Epoch [1/4], Step [10500/12500], Loss: 2.3022\n",
      "Epoch [1/4], Step [10600/12500], Loss: 2.2902\n",
      "Epoch [1/4], Step [10700/12500], Loss: 2.2995\n",
      "Epoch [1/4], Step [10800/12500], Loss: 2.3191\n",
      "Epoch [1/4], Step [10900/12500], Loss: 2.2853\n",
      "Epoch [1/4], Step [11000/12500], Loss: 2.3087\n",
      "Epoch [1/4], Step [11100/12500], Loss: 2.3073\n",
      "Epoch [1/4], Step [11200/12500], Loss: 2.3186\n",
      "Epoch [1/4], Step [11300/12500], Loss: 2.3065\n",
      "Epoch [1/4], Step [11400/12500], Loss: 2.2979\n",
      "Epoch [1/4], Step [11500/12500], Loss: 2.2918\n",
      "Epoch [1/4], Step [11600/12500], Loss: 2.3257\n",
      "Epoch [1/4], Step [11700/12500], Loss: 2.2709\n",
      "Epoch [1/4], Step [11800/12500], Loss: 2.3091\n",
      "Epoch [1/4], Step [11900/12500], Loss: 2.3188\n",
      "Epoch [1/4], Step [12000/12500], Loss: 2.3010\n",
      "Epoch [1/4], Step [12100/12500], Loss: 2.3089\n",
      "Epoch [1/4], Step [12200/12500], Loss: 2.3005\n",
      "Epoch [1/4], Step [12300/12500], Loss: 2.2698\n",
      "Epoch [1/4], Step [12400/12500], Loss: 2.2909\n",
      "Epoch [1/4], Step [12500/12500], Loss: 2.3026\n",
      "Epoch [2/4], Step [100/12500], Loss: 2.2854\n",
      "Epoch [2/4], Step [200/12500], Loss: 2.2894\n",
      "Epoch [2/4], Step [300/12500], Loss: 2.2923\n",
      "Epoch [2/4], Step [400/12500], Loss: 2.3001\n",
      "Epoch [2/4], Step [500/12500], Loss: 2.2941\n",
      "Epoch [2/4], Step [600/12500], Loss: 2.3095\n",
      "Epoch [2/4], Step [700/12500], Loss: 2.2674\n",
      "Epoch [2/4], Step [800/12500], Loss: 2.2842\n",
      "Epoch [2/4], Step [900/12500], Loss: 2.2656\n",
      "Epoch [2/4], Step [1000/12500], Loss: 2.2971\n",
      "Epoch [2/4], Step [1100/12500], Loss: 2.3008\n",
      "Epoch [2/4], Step [1200/12500], Loss: 2.2969\n",
      "Epoch [2/4], Step [1300/12500], Loss: 2.3054\n",
      "Epoch [2/4], Step [1400/12500], Loss: 2.2949\n",
      "Epoch [2/4], Step [1500/12500], Loss: 2.3203\n",
      "Epoch [2/4], Step [1600/12500], Loss: 2.3132\n",
      "Epoch [2/4], Step [1700/12500], Loss: 2.3160\n",
      "Epoch [2/4], Step [1800/12500], Loss: 2.2830\n",
      "Epoch [2/4], Step [1900/12500], Loss: 2.2823\n",
      "Epoch [2/4], Step [2000/12500], Loss: 2.3081\n",
      "Epoch [2/4], Step [2100/12500], Loss: 2.2893\n",
      "Epoch [2/4], Step [2200/12500], Loss: 2.2933\n",
      "Epoch [2/4], Step [2300/12500], Loss: 2.2946\n",
      "Epoch [2/4], Step [2400/12500], Loss: 2.2689\n",
      "Epoch [2/4], Step [2500/12500], Loss: 2.2978\n",
      "Epoch [2/4], Step [2600/12500], Loss: 2.2709\n",
      "Epoch [2/4], Step [2700/12500], Loss: 2.2917\n",
      "Epoch [2/4], Step [2800/12500], Loss: 2.2859\n",
      "Epoch [2/4], Step [2900/12500], Loss: 2.3249\n",
      "Epoch [2/4], Step [3000/12500], Loss: 2.3034\n",
      "Epoch [2/4], Step [3100/12500], Loss: 2.2299\n",
      "Epoch [2/4], Step [3200/12500], Loss: 2.2818\n",
      "Epoch [2/4], Step [3300/12500], Loss: 2.2650\n",
      "Epoch [2/4], Step [3400/12500], Loss: 2.2662\n",
      "Epoch [2/4], Step [3500/12500], Loss: 2.2138\n",
      "Epoch [2/4], Step [3600/12500], Loss: 2.2785\n",
      "Epoch [2/4], Step [3700/12500], Loss: 2.2345\n",
      "Epoch [2/4], Step [3800/12500], Loss: 2.2426\n",
      "Epoch [2/4], Step [3900/12500], Loss: 2.3153\n",
      "Epoch [2/4], Step [4000/12500], Loss: 2.3116\n",
      "Epoch [2/4], Step [4100/12500], Loss: 2.2798\n",
      "Epoch [2/4], Step [4200/12500], Loss: 2.2593\n",
      "Epoch [2/4], Step [4300/12500], Loss: 2.3443\n",
      "Epoch [2/4], Step [4400/12500], Loss: 2.3127\n",
      "Epoch [2/4], Step [4500/12500], Loss: 2.3065\n",
      "Epoch [2/4], Step [4600/12500], Loss: 2.2708\n",
      "Epoch [2/4], Step [4700/12500], Loss: 2.2703\n",
      "Epoch [2/4], Step [4800/12500], Loss: 2.3003\n",
      "Epoch [2/4], Step [4900/12500], Loss: 2.3288\n",
      "Epoch [2/4], Step [5000/12500], Loss: 2.3400\n",
      "Epoch [2/4], Step [5100/12500], Loss: 2.2921\n",
      "Epoch [2/4], Step [5200/12500], Loss: 2.2684\n",
      "Epoch [2/4], Step [5300/12500], Loss: 2.2603\n",
      "Epoch [2/4], Step [5400/12500], Loss: 2.3384\n",
      "Epoch [2/4], Step [5500/12500], Loss: 2.3014\n",
      "Epoch [2/4], Step [5600/12500], Loss: 2.2672\n",
      "Epoch [2/4], Step [5700/12500], Loss: 2.2077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/4], Step [5800/12500], Loss: 2.3412\n",
      "Epoch [2/4], Step [5900/12500], Loss: 2.2288\n",
      "Epoch [2/4], Step [6000/12500], Loss: 2.3218\n",
      "Epoch [2/4], Step [6100/12500], Loss: 2.3260\n",
      "Epoch [2/4], Step [6200/12500], Loss: 2.2956\n",
      "Epoch [2/4], Step [6300/12500], Loss: 2.2520\n",
      "Epoch [2/4], Step [6400/12500], Loss: 2.2821\n",
      "Epoch [2/4], Step [6500/12500], Loss: 2.2258\n",
      "Epoch [2/4], Step [6600/12500], Loss: 2.1782\n",
      "Epoch [2/4], Step [6700/12500], Loss: 2.2239\n",
      "Epoch [2/4], Step [6800/12500], Loss: 2.2248\n",
      "Epoch [2/4], Step [6900/12500], Loss: 2.2892\n",
      "Epoch [2/4], Step [7000/12500], Loss: 2.2837\n",
      "Epoch [2/4], Step [7100/12500], Loss: 2.3118\n",
      "Epoch [2/4], Step [7200/12500], Loss: 2.3206\n",
      "Epoch [2/4], Step [7300/12500], Loss: 2.1600\n",
      "Epoch [2/4], Step [7400/12500], Loss: 2.2949\n",
      "Epoch [2/4], Step [7500/12500], Loss: 2.3493\n",
      "Epoch [2/4], Step [7600/12500], Loss: 2.3497\n",
      "Epoch [2/4], Step [7700/12500], Loss: 2.2928\n",
      "Epoch [2/4], Step [7800/12500], Loss: 2.2566\n",
      "Epoch [2/4], Step [7900/12500], Loss: 2.3199\n",
      "Epoch [2/4], Step [8000/12500], Loss: 2.1454\n",
      "Epoch [2/4], Step [8100/12500], Loss: 2.3106\n",
      "Epoch [2/4], Step [8200/12500], Loss: 2.2458\n",
      "Epoch [2/4], Step [8300/12500], Loss: 2.2304\n",
      "Epoch [2/4], Step [8400/12500], Loss: 2.3411\n",
      "Epoch [2/4], Step [8500/12500], Loss: 2.2862\n",
      "Epoch [2/4], Step [8600/12500], Loss: 2.2685\n",
      "Epoch [2/4], Step [8700/12500], Loss: 2.3092\n",
      "Epoch [2/4], Step [8800/12500], Loss: 2.0841\n",
      "Epoch [2/4], Step [8900/12500], Loss: 2.2582\n",
      "Epoch [2/4], Step [9000/12500], Loss: 2.1557\n",
      "Epoch [2/4], Step [9100/12500], Loss: 2.1450\n",
      "Epoch [2/4], Step [9200/12500], Loss: 2.0998\n",
      "Epoch [2/4], Step [9300/12500], Loss: 2.2981\n",
      "Epoch [2/4], Step [9400/12500], Loss: 2.2105\n",
      "Epoch [2/4], Step [9500/12500], Loss: 2.1031\n",
      "Epoch [2/4], Step [9600/12500], Loss: 2.3244\n",
      "Epoch [2/4], Step [9700/12500], Loss: 2.2075\n",
      "Epoch [2/4], Step [9800/12500], Loss: 2.1793\n",
      "Epoch [2/4], Step [9900/12500], Loss: 2.2844\n",
      "Epoch [2/4], Step [10000/12500], Loss: 2.1496\n",
      "Epoch [2/4], Step [10100/12500], Loss: 2.3709\n",
      "Epoch [2/4], Step [10200/12500], Loss: 2.0940\n",
      "Epoch [2/4], Step [10300/12500], Loss: 2.1878\n",
      "Epoch [2/4], Step [10400/12500], Loss: 2.3932\n",
      "Epoch [2/4], Step [10500/12500], Loss: 2.2373\n",
      "Epoch [2/4], Step [10600/12500], Loss: 2.1700\n",
      "Epoch [2/4], Step [10700/12500], Loss: 2.1830\n",
      "Epoch [2/4], Step [10800/12500], Loss: 2.0943\n",
      "Epoch [2/4], Step [10900/12500], Loss: 2.1261\n",
      "Epoch [2/4], Step [11000/12500], Loss: 1.9676\n",
      "Epoch [2/4], Step [11100/12500], Loss: 2.1462\n",
      "Epoch [2/4], Step [11200/12500], Loss: 2.2097\n",
      "Epoch [2/4], Step [11300/12500], Loss: 2.0393\n",
      "Epoch [2/4], Step [11400/12500], Loss: 2.1688\n",
      "Epoch [2/4], Step [11500/12500], Loss: 2.3029\n",
      "Epoch [2/4], Step [11600/12500], Loss: 2.0878\n",
      "Epoch [2/4], Step [11700/12500], Loss: 2.2542\n",
      "Epoch [2/4], Step [11800/12500], Loss: 1.9605\n",
      "Epoch [2/4], Step [11900/12500], Loss: 1.8750\n",
      "Epoch [2/4], Step [12000/12500], Loss: 2.7502\n",
      "Epoch [2/4], Step [12100/12500], Loss: 2.0812\n",
      "Epoch [2/4], Step [12200/12500], Loss: 2.5705\n",
      "Epoch [2/4], Step [12300/12500], Loss: 1.8761\n",
      "Epoch [2/4], Step [12400/12500], Loss: 2.0417\n",
      "Epoch [2/4], Step [12500/12500], Loss: 2.2130\n",
      "Epoch [3/4], Step [100/12500], Loss: 2.1215\n",
      "Epoch [3/4], Step [200/12500], Loss: 2.1048\n",
      "Epoch [3/4], Step [300/12500], Loss: 2.0897\n",
      "Epoch [3/4], Step [400/12500], Loss: 2.3346\n",
      "Epoch [3/4], Step [500/12500], Loss: 2.1832\n",
      "Epoch [3/4], Step [600/12500], Loss: 2.1774\n",
      "Epoch [3/4], Step [700/12500], Loss: 2.3710\n",
      "Epoch [3/4], Step [800/12500], Loss: 1.7291\n",
      "Epoch [3/4], Step [900/12500], Loss: 2.0039\n",
      "Epoch [3/4], Step [1000/12500], Loss: 2.0736\n",
      "Epoch [3/4], Step [1100/12500], Loss: 2.2598\n",
      "Epoch [3/4], Step [1200/12500], Loss: 2.0240\n",
      "Epoch [3/4], Step [1300/12500], Loss: 1.9875\n",
      "Epoch [3/4], Step [1400/12500], Loss: 1.7390\n",
      "Epoch [3/4], Step [1500/12500], Loss: 2.8015\n",
      "Epoch [3/4], Step [1600/12500], Loss: 1.7816\n",
      "Epoch [3/4], Step [1700/12500], Loss: 2.0791\n",
      "Epoch [3/4], Step [1800/12500], Loss: 1.6162\n",
      "Epoch [3/4], Step [1900/12500], Loss: 1.9475\n",
      "Epoch [3/4], Step [2000/12500], Loss: 2.2130\n",
      "Epoch [3/4], Step [2100/12500], Loss: 2.2957\n",
      "Epoch [3/4], Step [2200/12500], Loss: 2.0544\n",
      "Epoch [3/4], Step [2300/12500], Loss: 2.9944\n",
      "Epoch [3/4], Step [2400/12500], Loss: 1.6359\n",
      "Epoch [3/4], Step [2500/12500], Loss: 1.9320\n",
      "Epoch [3/4], Step [2600/12500], Loss: 1.8084\n",
      "Epoch [3/4], Step [2700/12500], Loss: 2.0062\n",
      "Epoch [3/4], Step [2800/12500], Loss: 1.9499\n",
      "Epoch [3/4], Step [2900/12500], Loss: 1.7657\n",
      "Epoch [3/4], Step [3000/12500], Loss: 1.6472\n",
      "Epoch [3/4], Step [3100/12500], Loss: 2.3534\n",
      "Epoch [3/4], Step [3200/12500], Loss: 1.9181\n",
      "Epoch [3/4], Step [3300/12500], Loss: 2.4035\n",
      "Epoch [3/4], Step [3400/12500], Loss: 1.9719\n",
      "Epoch [3/4], Step [3500/12500], Loss: 1.6100\n",
      "Epoch [3/4], Step [3600/12500], Loss: 1.9141\n",
      "Epoch [3/4], Step [3700/12500], Loss: 3.7582\n",
      "Epoch [3/4], Step [3800/12500], Loss: 2.0522\n",
      "Epoch [3/4], Step [3900/12500], Loss: 1.6381\n",
      "Epoch [3/4], Step [4000/12500], Loss: 2.2523\n",
      "Epoch [3/4], Step [4100/12500], Loss: 2.6838\n",
      "Epoch [3/4], Step [4200/12500], Loss: 1.5648\n",
      "Epoch [3/4], Step [4300/12500], Loss: 1.4588\n",
      "Epoch [3/4], Step [4400/12500], Loss: 2.2814\n",
      "Epoch [3/4], Step [4500/12500], Loss: 2.2042\n",
      "Epoch [3/4], Step [4600/12500], Loss: 1.6040\n",
      "Epoch [3/4], Step [4700/12500], Loss: 1.7744\n",
      "Epoch [3/4], Step [4800/12500], Loss: 2.0322\n",
      "Epoch [3/4], Step [4900/12500], Loss: 3.1793\n",
      "Epoch [3/4], Step [5000/12500], Loss: 2.1623\n",
      "Epoch [3/4], Step [5100/12500], Loss: 1.8639\n",
      "Epoch [3/4], Step [5200/12500], Loss: 2.3694\n",
      "Epoch [3/4], Step [5300/12500], Loss: 1.7763\n",
      "Epoch [3/4], Step [5400/12500], Loss: 1.7409\n",
      "Epoch [3/4], Step [5500/12500], Loss: 1.9098\n",
      "Epoch [3/4], Step [5600/12500], Loss: 1.8868\n",
      "Epoch [3/4], Step [5700/12500], Loss: 2.0347\n",
      "Epoch [3/4], Step [5800/12500], Loss: 1.8014\n",
      "Epoch [3/4], Step [5900/12500], Loss: 2.0487\n",
      "Epoch [3/4], Step [6000/12500], Loss: 1.9508\n",
      "Epoch [3/4], Step [6100/12500], Loss: 1.8500\n",
      "Epoch [3/4], Step [6200/12500], Loss: 1.7456\n",
      "Epoch [3/4], Step [6300/12500], Loss: 2.0704\n",
      "Epoch [3/4], Step [6400/12500], Loss: 1.9067\n",
      "Epoch [3/4], Step [6500/12500], Loss: 1.6692\n",
      "Epoch [3/4], Step [6600/12500], Loss: 2.0519\n",
      "Epoch [3/4], Step [6700/12500], Loss: 2.3462\n",
      "Epoch [3/4], Step [6800/12500], Loss: 1.6675\n",
      "Epoch [3/4], Step [6900/12500], Loss: 2.1653\n",
      "Epoch [3/4], Step [7000/12500], Loss: 1.5961\n",
      "Epoch [3/4], Step [7100/12500], Loss: 2.2572\n",
      "Epoch [3/4], Step [7200/12500], Loss: 2.4865\n",
      "Epoch [3/4], Step [7300/12500], Loss: 1.7230\n",
      "Epoch [3/4], Step [7400/12500], Loss: 2.0379\n",
      "Epoch [3/4], Step [7500/12500], Loss: 1.6965\n",
      "Epoch [3/4], Step [7600/12500], Loss: 2.0513\n",
      "Epoch [3/4], Step [7700/12500], Loss: 2.3167\n",
      "Epoch [3/4], Step [7800/12500], Loss: 1.6085\n",
      "Epoch [3/4], Step [7900/12500], Loss: 2.1117\n",
      "Epoch [3/4], Step [8000/12500], Loss: 1.9048\n",
      "Epoch [3/4], Step [8100/12500], Loss: 2.3551\n",
      "Epoch [3/4], Step [8200/12500], Loss: 2.0352\n",
      "Epoch [3/4], Step [8300/12500], Loss: 1.7356\n",
      "Epoch [3/4], Step [8400/12500], Loss: 1.9637\n",
      "Epoch [3/4], Step [8500/12500], Loss: 1.7782\n",
      "Epoch [3/4], Step [8600/12500], Loss: 2.1957\n",
      "Epoch [3/4], Step [8700/12500], Loss: 1.9183\n",
      "Epoch [3/4], Step [8800/12500], Loss: 1.7883\n",
      "Epoch [3/4], Step [8900/12500], Loss: 1.7624\n",
      "Epoch [3/4], Step [9000/12500], Loss: 1.6898\n",
      "Epoch [3/4], Step [9100/12500], Loss: 1.8578\n",
      "Epoch [3/4], Step [9200/12500], Loss: 1.1539\n",
      "Epoch [3/4], Step [9300/12500], Loss: 1.9954\n",
      "Epoch [3/4], Step [9400/12500], Loss: 1.5306\n",
      "Epoch [3/4], Step [9500/12500], Loss: 1.9500\n",
      "Epoch [3/4], Step [9600/12500], Loss: 2.0500\n",
      "Epoch [3/4], Step [9700/12500], Loss: 2.3874\n",
      "Epoch [3/4], Step [9800/12500], Loss: 2.4131\n",
      "Epoch [3/4], Step [9900/12500], Loss: 1.7433\n",
      "Epoch [3/4], Step [10000/12500], Loss: 2.6697\n",
      "Epoch [3/4], Step [10100/12500], Loss: 1.4049\n",
      "Epoch [3/4], Step [10200/12500], Loss: 1.6763\n",
      "Epoch [3/4], Step [10300/12500], Loss: 1.9924\n",
      "Epoch [3/4], Step [10400/12500], Loss: 1.7812\n",
      "Epoch [3/4], Step [10500/12500], Loss: 2.0995\n",
      "Epoch [3/4], Step [10600/12500], Loss: 2.4072\n",
      "Epoch [3/4], Step [10700/12500], Loss: 1.7516\n",
      "Epoch [3/4], Step [10800/12500], Loss: 2.0881\n",
      "Epoch [3/4], Step [10900/12500], Loss: 2.5319\n",
      "Epoch [3/4], Step [11000/12500], Loss: 1.5615\n",
      "Epoch [3/4], Step [11100/12500], Loss: 1.8454\n",
      "Epoch [3/4], Step [11200/12500], Loss: 2.0277\n",
      "Epoch [3/4], Step [11300/12500], Loss: 2.2154\n",
      "Epoch [3/4], Step [11400/12500], Loss: 1.5567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/4], Step [11500/12500], Loss: 1.6121\n",
      "Epoch [3/4], Step [11600/12500], Loss: 1.7283\n",
      "Epoch [3/4], Step [11700/12500], Loss: 2.3154\n",
      "Epoch [3/4], Step [11800/12500], Loss: 1.9506\n",
      "Epoch [3/4], Step [11900/12500], Loss: 2.1849\n",
      "Epoch [3/4], Step [12000/12500], Loss: 2.7859\n",
      "Epoch [3/4], Step [12100/12500], Loss: 1.9221\n",
      "Epoch [3/4], Step [12200/12500], Loss: 1.5598\n",
      "Epoch [3/4], Step [12300/12500], Loss: 1.6588\n",
      "Epoch [3/4], Step [12400/12500], Loss: 1.5441\n",
      "Epoch [3/4], Step [12500/12500], Loss: 2.1955\n",
      "Epoch [4/4], Step [100/12500], Loss: 1.8173\n",
      "Epoch [4/4], Step [200/12500], Loss: 1.9673\n",
      "Epoch [4/4], Step [300/12500], Loss: 2.1380\n",
      "Epoch [4/4], Step [400/12500], Loss: 1.9088\n",
      "Epoch [4/4], Step [500/12500], Loss: 1.7941\n",
      "Epoch [4/4], Step [600/12500], Loss: 1.7830\n",
      "Epoch [4/4], Step [700/12500], Loss: 2.4404\n",
      "Epoch [4/4], Step [800/12500], Loss: 1.8661\n",
      "Epoch [4/4], Step [900/12500], Loss: 1.8951\n",
      "Epoch [4/4], Step [1000/12500], Loss: 1.8616\n",
      "Epoch [4/4], Step [1100/12500], Loss: 1.9073\n",
      "Epoch [4/4], Step [1200/12500], Loss: 1.3725\n",
      "Epoch [4/4], Step [1300/12500], Loss: 1.8531\n",
      "Epoch [4/4], Step [1400/12500], Loss: 1.7832\n",
      "Epoch [4/4], Step [1500/12500], Loss: 1.9664\n",
      "Epoch [4/4], Step [1600/12500], Loss: 1.7270\n",
      "Epoch [4/4], Step [1700/12500], Loss: 1.6377\n",
      "Epoch [4/4], Step [1800/12500], Loss: 1.7070\n",
      "Epoch [4/4], Step [1900/12500], Loss: 2.1518\n",
      "Epoch [4/4], Step [2000/12500], Loss: 2.0413\n",
      "Epoch [4/4], Step [2100/12500], Loss: 1.3498\n",
      "Epoch [4/4], Step [2200/12500], Loss: 2.4334\n",
      "Epoch [4/4], Step [2300/12500], Loss: 1.8026\n",
      "Epoch [4/4], Step [2400/12500], Loss: 1.9818\n",
      "Epoch [4/4], Step [2500/12500], Loss: 1.6190\n",
      "Epoch [4/4], Step [2600/12500], Loss: 1.5107\n",
      "Epoch [4/4], Step [2700/12500], Loss: 2.1657\n",
      "Epoch [4/4], Step [2800/12500], Loss: 1.6733\n",
      "Epoch [4/4], Step [2900/12500], Loss: 2.1193\n",
      "Epoch [4/4], Step [3000/12500], Loss: 1.7657\n",
      "Epoch [4/4], Step [3100/12500], Loss: 1.7931\n",
      "Epoch [4/4], Step [3200/12500], Loss: 1.9375\n",
      "Epoch [4/4], Step [3300/12500], Loss: 1.8000\n",
      "Epoch [4/4], Step [3400/12500], Loss: 1.6704\n",
      "Epoch [4/4], Step [3500/12500], Loss: 1.9279\n",
      "Epoch [4/4], Step [3600/12500], Loss: 2.3266\n",
      "Epoch [4/4], Step [3700/12500], Loss: 2.1115\n",
      "Epoch [4/4], Step [3800/12500], Loss: 1.9596\n",
      "Epoch [4/4], Step [3900/12500], Loss: 3.5371\n",
      "Epoch [4/4], Step [4000/12500], Loss: 1.8671\n",
      "Epoch [4/4], Step [4100/12500], Loss: 1.7283\n",
      "Epoch [4/4], Step [4200/12500], Loss: 1.8230\n",
      "Epoch [4/4], Step [4300/12500], Loss: 1.3476\n",
      "Epoch [4/4], Step [4400/12500], Loss: 1.7095\n",
      "Epoch [4/4], Step [4500/12500], Loss: 2.2099\n",
      "Epoch [4/4], Step [4600/12500], Loss: 2.1278\n",
      "Epoch [4/4], Step [4700/12500], Loss: 2.3058\n",
      "Epoch [4/4], Step [4800/12500], Loss: 1.8015\n",
      "Epoch [4/4], Step [4900/12500], Loss: 2.1826\n",
      "Epoch [4/4], Step [5000/12500], Loss: 1.9017\n",
      "Epoch [4/4], Step [5100/12500], Loss: 2.3820\n",
      "Epoch [4/4], Step [5200/12500], Loss: 2.1173\n",
      "Epoch [4/4], Step [5300/12500], Loss: 1.5576\n",
      "Epoch [4/4], Step [5400/12500], Loss: 2.4035\n",
      "Epoch [4/4], Step [5500/12500], Loss: 2.0494\n",
      "Epoch [4/4], Step [5600/12500], Loss: 2.3765\n",
      "Epoch [4/4], Step [5700/12500], Loss: 1.4233\n",
      "Epoch [4/4], Step [5800/12500], Loss: 2.7597\n",
      "Epoch [4/4], Step [5900/12500], Loss: 2.1234\n",
      "Epoch [4/4], Step [6000/12500], Loss: 1.5260\n",
      "Epoch [4/4], Step [6100/12500], Loss: 2.1185\n",
      "Epoch [4/4], Step [6200/12500], Loss: 1.3880\n",
      "Epoch [4/4], Step [6300/12500], Loss: 2.1432\n",
      "Epoch [4/4], Step [6400/12500], Loss: 1.7063\n",
      "Epoch [4/4], Step [6500/12500], Loss: 1.5962\n",
      "Epoch [4/4], Step [6600/12500], Loss: 1.3488\n",
      "Epoch [4/4], Step [6700/12500], Loss: 1.4107\n",
      "Epoch [4/4], Step [6800/12500], Loss: 2.0764\n",
      "Epoch [4/4], Step [6900/12500], Loss: 1.9876\n",
      "Epoch [4/4], Step [7000/12500], Loss: 2.0382\n",
      "Epoch [4/4], Step [7100/12500], Loss: 1.7526\n",
      "Epoch [4/4], Step [7200/12500], Loss: 1.5274\n",
      "Epoch [4/4], Step [7300/12500], Loss: 1.5378\n",
      "Epoch [4/4], Step [7400/12500], Loss: 1.3825\n",
      "Epoch [4/4], Step [7500/12500], Loss: 1.6890\n",
      "Epoch [4/4], Step [7600/12500], Loss: 2.0061\n",
      "Epoch [4/4], Step [7700/12500], Loss: 1.5503\n",
      "Epoch [4/4], Step [7800/12500], Loss: 1.4975\n",
      "Epoch [4/4], Step [7900/12500], Loss: 2.3853\n",
      "Epoch [4/4], Step [8000/12500], Loss: 1.5237\n",
      "Epoch [4/4], Step [8100/12500], Loss: 1.8026\n",
      "Epoch [4/4], Step [8200/12500], Loss: 1.2091\n",
      "Epoch [4/4], Step [8300/12500], Loss: 1.6719\n",
      "Epoch [4/4], Step [8400/12500], Loss: 1.8791\n",
      "Epoch [4/4], Step [8500/12500], Loss: 1.7032\n",
      "Epoch [4/4], Step [8600/12500], Loss: 1.1919\n",
      "Epoch [4/4], Step [8700/12500], Loss: 1.9427\n",
      "Epoch [4/4], Step [8800/12500], Loss: 1.3839\n",
      "Epoch [4/4], Step [8900/12500], Loss: 1.9692\n",
      "Epoch [4/4], Step [9000/12500], Loss: 1.7949\n",
      "Epoch [4/4], Step [9100/12500], Loss: 1.9994\n",
      "Epoch [4/4], Step [9200/12500], Loss: 1.8383\n",
      "Epoch [4/4], Step [9300/12500], Loss: 2.0713\n",
      "Epoch [4/4], Step [9400/12500], Loss: 1.4118\n",
      "Epoch [4/4], Step [9500/12500], Loss: 2.0423\n",
      "Epoch [4/4], Step [9600/12500], Loss: 2.1671\n",
      "Epoch [4/4], Step [9700/12500], Loss: 2.0206\n",
      "Epoch [4/4], Step [9800/12500], Loss: 1.5130\n",
      "Epoch [4/4], Step [9900/12500], Loss: 1.7671\n",
      "Epoch [4/4], Step [10000/12500], Loss: 1.4990\n",
      "Epoch [4/4], Step [10100/12500], Loss: 2.1828\n",
      "Epoch [4/4], Step [10200/12500], Loss: 1.7443\n",
      "Epoch [4/4], Step [10300/12500], Loss: 1.6731\n",
      "Epoch [4/4], Step [10400/12500], Loss: 2.4072\n",
      "Epoch [4/4], Step [10500/12500], Loss: 1.5053\n",
      "Epoch [4/4], Step [10600/12500], Loss: 1.3596\n",
      "Epoch [4/4], Step [10700/12500], Loss: 1.9078\n",
      "Epoch [4/4], Step [10800/12500], Loss: 1.8139\n",
      "Epoch [4/4], Step [10900/12500], Loss: 2.0643\n",
      "Epoch [4/4], Step [11000/12500], Loss: 1.4214\n",
      "Epoch [4/4], Step [11100/12500], Loss: 1.6644\n",
      "Epoch [4/4], Step [11200/12500], Loss: 1.9591\n",
      "Epoch [4/4], Step [11300/12500], Loss: 1.9634\n",
      "Epoch [4/4], Step [11400/12500], Loss: 1.9927\n",
      "Epoch [4/4], Step [11500/12500], Loss: 1.1083\n",
      "Epoch [4/4], Step [11600/12500], Loss: 1.4550\n",
      "Epoch [4/4], Step [11700/12500], Loss: 1.5691\n",
      "Epoch [4/4], Step [11800/12500], Loss: 1.8209\n",
      "Epoch [4/4], Step [11900/12500], Loss: 1.2353\n",
      "Epoch [4/4], Step [12000/12500], Loss: 1.5631\n",
      "Epoch [4/4], Step [12100/12500], Loss: 1.6982\n",
      "Epoch [4/4], Step [12200/12500], Loss: 1.6690\n",
      "Epoch [4/4], Step [12300/12500], Loss: 1.8647\n",
      "Epoch [4/4], Step [12400/12500], Loss: 1.7160\n",
      "Epoch [4/4], Step [12500/12500], Loss: 1.2229\n",
      "Finished Training!\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1,16*5*5) # flaten -1 for the batchsize\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        # no activtion function, no softmax\n",
    "        return x\n",
    "    \n",
    "model = ConvNet().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_toatl_steps = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # forardpass \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_toatl_steps}], Loss: {loss.item():.4f}')\n",
    "            \n",
    "            \n",
    "print('Finished Training!')\n",
    "\n",
    "\n",
    "# for saving the model\n",
    "#PATH = './cnn.pth'\n",
    "#torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 34.64 %\n",
      "Accuracy of plane: 43.3 %\n",
      "Accuracy of car: 34.0 %\n",
      "Accuracy of bird: 15.7 %\n",
      "Accuracy of cat: 9.6 %\n",
      "Accuracy of deer: 7.5 %\n",
      "Accuracy of dog: 41.7 %\n",
      "Accuracy of frog: 71.0 %\n",
      "Accuracy of horse: 48.4 %\n",
      "Accuracy of ship: 26.7 %\n",
      "Accuracy of truck: 48.5 %\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "# we do not compute the grad in evaluation\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # predict\n",
    "        outputs = model(images)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "    for i in range(10):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
